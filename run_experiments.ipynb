{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from slim_gsgp.main_mo_gp import mo_gp\n",
    "from slim_gsgp.datasets.data_loader import (\n",
    "     load_efficiency_cooling, load_ld50, \n",
    "     #load_boston, \n",
    ")\n",
    "\n",
    "# Configuration\n",
    "RANDOM_SEED = 37\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Datasets to Test (we need to import them first)\n",
    "DATASETS = {\n",
    "    'Toxicity': load_ld50,\n",
    "    'Cooling': load_efficiency_cooling, \n",
    "}\n",
    "\n",
    "\n",
    "# Objective Configurations\n",
    "OBJECTIVE_SETS = {\n",
    "    '2_Objs': {\n",
    "        'funcs': [\"rmse\", \"size\"],\n",
    "        'flags': [True, True] # Min, Min\n",
    "    },\n",
    "    '3_Objs': {\n",
    "        'funcs': [\"rmse\", \"size\", \"features\"],\n",
    "        'flags': [True, True, True]\n",
    "    },\n",
    "    '5_Objs': {\n",
    "        'funcs': [\"rmse\", \"size\", \"features\", \"nao\", \"naoc\"],\n",
    "        'flags': [True, True, True, True, True]\n",
    "    }\n",
    "}\n",
    "\n",
    "SCENARIOS = {\n",
    "    'NSGA-II_Pure': {\n",
    "        'selector': 'nsga2',\n",
    "        'survival': 'nsga2',\n",
    "        'n_elites': 0, \n",
    "        'elitism_strategy': 'nsga2'\n",
    "    },\n",
    "    'NT_NSGA-II': {\n",
    "        'selector': 'nested_tournament',\n",
    "        'survival': 'nsga2',\n",
    "        'n_elites': 0,\n",
    "        'elitism_strategy': 'nsga2'\n",
    "    },\n",
    "    'NT_No_Elitism': {\n",
    "        'selector': 'nested_tournament',\n",
    "        'survival': 'generational',\n",
    "        'n_elites': 0,\n",
    "        'elitism_strategy': 'nsga2'\n",
    "    },\n",
    "    'NT_1st_Obj_Elitism': {\n",
    "        'selector': 'nested_tournament',\n",
    "        'survival': 'generational',\n",
    "        'n_elites': 1,\n",
    "        'elitism_strategy': 'first_obj' #uses RMSE\n",
    "    },\n",
    "    'NT_Rank_CD_Elitism': {\n",
    "        'selector': 'nested_tournament',\n",
    "        'survival': 'generational',\n",
    "        'n_elites': 1,\n",
    "        'elitism_strategy': 'nsga2' #Rank + CD\n",
    "    },\n",
    "    'NT_Ideal_Cand_Elitism': {\n",
    "        'selector': 'nested_tournament',\n",
    "        'survival': 'generational',\n",
    "        'n_elites': 1,\n",
    "        'elitism_strategy': 'ideal_point' #dynamic ideal point\n",
    "    }\n",
    "}\n",
    "\n",
    "#Hyperparameter Grid for Inner CV\n",
    "FIXED_PARAMS = {\n",
    "    'pop_size': 400,\n",
    "    'n_iter': 250,\n",
    "    'p_xo': 0.8,           #default  \n",
    "    'prob_const': 0.2,     #default  \n",
    "    'max_depth': 17,       #default  \n",
    "    'initializer': 'rhh',  #default\n",
    "    \"init_depth\": 4,\n",
    "    \"seed\": 74,            #default\n",
    "    \"n_jobs\": 1,           #default\n",
    "    \"test_elite\": True    \n",
    "}\n",
    "\n",
    "#usado para teste pequeno\n",
    "# FIXED_PARAMS = {\n",
    "#     'pop_size': 10,\n",
    "#     'n_iter': 5,\n",
    "#     'p_xo': 0.8,\n",
    "#     'prob_const': 0.2,    \n",
    "#     'max_depth': 6,\n",
    "#     'initializer': 'rhh', \n",
    "#     \"init_depth\": 6,      \n",
    "#     \"seed\": 74,           \n",
    "#     \"n_jobs\": 1,\n",
    "#     \"test_elite\": True    \n",
    "# }\n",
    "\n",
    "##################################JUST COMMENTS##################################\n",
    "#Other parameters will remain default:\n",
    "###offspring_size###\n",
    "# if offspring_size is None:\n",
    "#     n_offspring = self.pop_size\n",
    "\n",
    "###n_elites###\n",
    "# it depends on the scenario being used\n",
    "\n",
    "###log_path & log_level###\n",
    "#posso criar um log =5 e fazer hard coded o que quero armazenar    APAGAR\n",
    "# it depends on dataset, scenario, fold, hyperparams\n",
    "# APAGAR: find out what information we need to log (its better to log more info and then ignore what is not needed than the opposite)\n",
    "\n",
    "###fitness_functions, minimization_flags###\n",
    "#it depends on the scenario being used: OBJECTIVE_SETS (Ok)\n",
    "\n",
    "###tournament_sizes###\n",
    "# Tournament sizes need to be dynamic based on n_objectives, handled in loop. When using Nested Tournament Selection, but for now we will fix it \n",
    "\n",
    "###ideal_candidate_values###\n",
    "# it can no longer be user defined      \n",
    "\n",
    "### \"test_elite\": True###\n",
    "\n",
    "### tree_functions ###\n",
    "# FUNCTIONS = {\n",
    "#     'add': {'function': torch.add, 'arity': 2},\n",
    "#     'subtract': {'function': torch.sub, 'arity': 2},\n",
    "#     'multiply': {'function': torch.mul, 'arity': 2},\n",
    "#     'divide': {'function': utils.protected_div, 'arity': 2},\n",
    "#     'mod': {'function': utils.protected_mod, 'arity': 2},\n",
    "#     'pow': {'function': utils.protected_pow, 'arity': 2},\n",
    "# }\n",
    "\n",
    "### tree_constants ###\n",
    "# I changed what originaly was in gp_config.py to the following:\n",
    "# random.seed(47)\n",
    "# CONSTANTS = {\n",
    "#     f'constant_{i}': lambda _, val=random.uniform(-1, 1): torch.tensor(val)\n",
    "#     for i in range(10)\n",
    "# }\n",
    "#################################################################################\n",
    "\n",
    "\n",
    "N_SPLITS_MC = 30 \n",
    "TEST_SIZE = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#e5c120ff; padding:1px; border-radius:10px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import ShuffleSplit\n",
    "# TEST_SIZE = 0.3\n",
    "#APAGAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each dataset: load data\n",
    "    # For each Outer fold: (14 folds for train + 1 fold for test each time) -> what will be use to track scores\n",
    "        # For each objective structure (Obj: 2, 3, 5)\n",
    "            # For each Scenario\n",
    "                # For each combination of current_grid# Estrutura: \n",
    "    \n",
    "#./log_mo/DATASET/SCENARIO/OBJECTIVES/fold_X.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Dataset: Toxicity\n",
      "========================================\n",
      "\n",
      "  > MC Split 1/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 2/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 3/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 4/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 5/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 6/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 7/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 8/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 9/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 10/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 11/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 12/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 13/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 14/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 15/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 16/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 17/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 18/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 19/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 20/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 21/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 22/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 23/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 24/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 25/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 26/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 27/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 28/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 29/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 30/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "========================================\n",
      "Dataset: Cooling\n",
      "========================================\n",
      "\n",
      "  > MC Split 1/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 2/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 3/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 4/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 5/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 6/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 7/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 8/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 9/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 10/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 11/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 12/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 13/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 14/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 15/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 16/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 17/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 18/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 19/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 20/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 21/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 22/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 23/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 24/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 25/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 26/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 27/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 28/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 29/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n",
      "\n",
      "  > MC Split 30/30\n",
      "    > Objectives: 2_Objs\n",
      "    > Objectives: 3_Objs\n",
      "    > Objectives: 5_Objs\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    for ds_name, loader_func in DATASETS.items():\n",
    "        print(f\"\\n{'='*40}\\nDataset: {ds_name}\\n{'='*40}\")\n",
    "    \n",
    "        # Load Data (X: features, y: target)\n",
    "        X, y = loader_func(X_y=True)\n",
    "    \n",
    "        # Monte Carlo CV (30 random splits)\n",
    "        rs = ShuffleSplit(n_splits=N_SPLITS_MC, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
    "    \n",
    "        for split_idx, (train_idx, test_idx) in enumerate(rs.split(X, y)):\n",
    "            print(f\"\\n  > MC Split {split_idx+1}/{N_SPLITS_MC}\")\n",
    "\n",
    "            X_train = X[train_idx]\n",
    "            y_train = y[train_idx]\n",
    "            X_test = X[test_idx]\n",
    "            y_test = y[test_idx]\n",
    "\n",
    "            for obj_set_name, obj_config in OBJECTIVE_SETS.items():\n",
    "                n_objs = len(obj_config['funcs'])\n",
    "                print(f\"    > Objectives: {obj_set_name}\")\n",
    "                \n",
    "                for scen_name, scen_config in SCENARIOS.items():\n",
    "                    # print(f\"      > Scenario: {scen_name}\")\n",
    "                    \n",
    "                    fold_dir = f\"./log_mo/{ds_name}/{scen_name}/{obj_set_name}/fold_{split_idx+1}\"\n",
    "                    \n",
    "                    if not os.path.exists(fold_dir): os.makedirs(fold_dir)\n",
    "                    \n",
    "                    log_path = os.path.join(fold_dir, \"execution_log.csv\")\n",
    "\n",
    "                    #if we already ran it, skip\n",
    "                    if os.path.exists(log_path):\n",
    "                            continue\n",
    "                    \n",
    "                    t_sizes_final = [2] * n_objs\n",
    "\n",
    "                    try:\n",
    "                        mo_gp(\n",
    "                            X_train=X_train, y_train=y_train,\n",
    "                            X_test=X_test, y_test=y_test,\n",
    "                            dataset_name=ds_name,\n",
    "                            fitness_functions=obj_config['funcs'],\n",
    "                            minimization_flags=obj_config['flags'],\n",
    "                            tournament_sizes=t_sizes_final,\n",
    "                            elitism_strategy=scen_config['elitism_strategy'],\n",
    "                            selector_strategy=scen_config['selector'],\n",
    "                            survival_strategy=scen_config['survival'],\n",
    "                            n_elites=scen_config['n_elites'],\n",
    "                                \n",
    "                            #fixed params\n",
    "                            pop_size=FIXED_PARAMS['pop_size'],\n",
    "                            n_iter=FIXED_PARAMS['n_iter'],\n",
    "                            p_xo=FIXED_PARAMS['p_xo'],\n",
    "                            prob_const=FIXED_PARAMS['prob_const'],\n",
    "                            max_depth=FIXED_PARAMS['max_depth'],\n",
    "                            initializer=FIXED_PARAMS['initializer'],\n",
    "                            init_depth=FIXED_PARAMS['init_depth'],\n",
    "                            verbose=0, \n",
    "                            log_level=5,\n",
    "                            log_path=log_path,\n",
    "                            n_jobs=FIXED_PARAMS['n_jobs'],\n",
    "                            test_elite=FIXED_PARAMS['test_elite']\n",
    "                        )\n",
    "                        print(f\"        > Log saved: {scen_name} @ Split {split_idx+1}\")\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                            print(f\"      [Error] {ds_name} {scen_name} Split {split_idx+1}: {e}\")\n",
    "                            import traceback\n",
    "                            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #it won't be use anymore because we will fix the best hyperparameters for each dataset, scenario and objective set already in the first outer fold\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     for ds_name, loader_func in DATASETS.items():\n",
    "#         print(f\"\\n{'='*40}\\nDataset: {ds_name}\\n{'='*40}\")\n",
    "    \n",
    "#         # Load Data (X: features, y: target)\n",
    "#         X, y = loader_func(X_y=True)\n",
    "    \n",
    "#         # Outer CV (14 folds for train + 1 fold for test each time)\n",
    "#         kf_outer = KFold(n_splits=K_OUTER, shuffle=True, random_state=RANDOM_SEED)\n",
    "    \n",
    "#         for outer_fold, (train_idx, test_idx) in enumerate(kf_outer.split(X, y)):\n",
    "#             print(f\"\\n  > Outer Fold {outer_fold+1}/{K_OUTER}\")\n",
    "        \n",
    "#             X_train_outer = X[train_idx]\n",
    "#             y_train_outer = y[train_idx]\n",
    "#             X_test_outer = X[test_idx]\n",
    "#             y_test_outer = y[test_idx]\n",
    "        \n",
    "#             # Inner CV Setup\n",
    "#             kf_inner = KFold(n_splits=K_INNER, shuffle=True, random_state=RANDOM_SEED)\n",
    "        \n",
    "#             for obj_set_name, obj_config in OBJECTIVE_SETS.items():\n",
    "#                 n_objs = len(obj_config['funcs'])\n",
    "#                 print(f\"    > Objectives: {obj_set_name}\")\n",
    "            \n",
    "#                 for scen_name, scen_config in SCENARIOS.items():\n",
    "#                     print(f\"      > Scenario: {scen_name}\")\n",
    "                \n",
    "#                     # Hyperparameter Tuning (Inner CV)\n",
    "#                     best_params = None\n",
    "#                     best_inner_score = float('inf') # Min RMSE of validation\n",
    "                \n",
    "#                     # All possible parameter configurations\n",
    "#                     current_grid = list(ParameterGrid(PARAM_GRID))\n",
    "                \n",
    "#                     for params in current_grid:\n",
    "#                         # tournament sizes probably has a big impact but let's leave it constant for now\n",
    "#                         t_sizes = [2] * n_objs\n",
    "\n",
    "#                         inner_rmse_scores = []\n",
    "\n",
    "#                         # Loop Inner CV: trains in (k_inner -1) folds, validates in 1 fold\n",
    "#                         for inner_t_idx, inner_v_idx in kf_inner.split(X_train_outer, y_train_outer):\n",
    "#                             X_in_t, y_in_t = X_train_outer[inner_t_idx], y_train_outer[inner_t_idx]\n",
    "#                             X_in_v, y_in_v = X_train_outer[inner_v_idx], y_train_outer[inner_v_idx]\n",
    "                        \n",
    "#                             # Run MOGP (I suppress output for inner loops)\n",
    "#                             try:\n",
    "#                                 elite = mo_gp(\n",
    "#                                     X_train=X_in_t, y_train=y_in_t,\n",
    "#                                     X_test=X_in_v, y_test=y_in_v,\n",
    "#                                     dataset_name=f\"{ds_name}_inner\",\n",
    "#                                     fitness_functions=obj_config['funcs'],\n",
    "#                                     minimization_flags=obj_config['flags'],\n",
    "#                                     tournament_sizes=t_sizes,\n",
    "#                                     elitism_strategy=scen_config['elitism_strategy'],\n",
    "#                                     selector_strategy=scen_config['selector'],\n",
    "#                                     survival_strategy=scen_config['survival'],\n",
    "#                                     n_elites=scen_config['n_elites'],\n",
    "#                                     log_level=0, verbose=0, **params # No logging for inner\n",
    "#                                 )\n",
    "#                                 # validation RMSE:find_mo_elites_default returns elite with test_fitness already calculated  \n",
    "#                                 if elite.test_fitness is not None:\n",
    "#                                     val_rmse = float(elite.test_fitness[0])\n",
    "#                                 else:\n",
    "#                                     val_rmse = float('inf')\n",
    "#                                 inner_rmse_scores.append(val_rmse)\n",
    "\n",
    "#                             except Exception as e:\n",
    "#                                 # ############# APAGAR\n",
    "#                                 # print(f\"        [Inner Error] {e}\") \n",
    "#                                 # import traceback\n",
    "#                                 # traceback.print_exc()\n",
    "#                                 # #############\n",
    "#                                 # for debug\n",
    "#                                 inner_rmse_scores.append(float('inf'))\n",
    "                    \n",
    "#                         # The winner is the one with the lowest median RMSE in validation\n",
    "#                         median_rmse = np.median(inner_rmse_scores)\n",
    "                    \n",
    "#                         if median_rmse < best_inner_score:\n",
    "#                             best_inner_score = median_rmse\n",
    "#                             best_params = params.copy()\n",
    "#                             best_params['tournament_sizes'] = t_sizes   # Store the calculated list \n",
    "\n",
    "#                     # #########DEBUG######### APAGAR\n",
    "#                     # if best_params is None:\n",
    "#                     #     print(f\"        [WARNING] All inner runs failed for {scen_name}. Skipping outer run.\")\n",
    "#                     #     continue\n",
    "#                     # #######################\n",
    "\n",
    "#                     # Setup Log Directory: ./log_mo / Dataset / Scenario / Objetivos / fold_X /\n",
    "#                     fold_dir = f\"./log_mo/{ds_name}/{scen_name}/{obj_set_name}/fold_{outer_fold+1}\"\n",
    "#                     if not os.path.exists(fold_dir):\n",
    "#                         os.makedirs(fold_dir)\n",
    "                \n",
    "#                     # Save CSV of Winner Hyperparameters\n",
    "#                     # remove tournament_sizes from csv to make it cleaner\n",
    "#                     if 'tournament_sizes' in best_params: del best_params['tournament_sizes'] \n",
    "                \n",
    "#                     #just to have the inner score saved\n",
    "#                     best_params['Median_Inner_RMSE'] = best_inner_score\n",
    "\n",
    "#                     pd.DataFrame([best_params]).to_csv(os.path.join(fold_dir, \"best_inner_params.csv\"), index=False)\n",
    "                \n",
    "#                     print(f\"        > Best Params: {best_params} (RMSE Val: {best_inner_score:.4f})\")\n",
    "\n",
    "#                     # final training with best hyperparameters on the outer fold\n",
    "#                     log_path = os.path.join(fold_dir, \"execution_log.csv\")\n",
    "\n",
    "#                     t_sizes_final = [2] * n_objs #needs to be recalculated here again because I dropped it before\n",
    "                \n",
    "#                     try:\n",
    "#                         mo_gp(\n",
    "#                             X_train=X_train_outer, y_train=y_train_outer,\n",
    "#                             X_test=X_test_outer, y_test=y_test_outer,\n",
    "#                             dataset_name=ds_name,\n",
    "#                             fitness_functions=obj_config['funcs'],\n",
    "#                             minimization_flags=obj_config['flags'],\n",
    "#                             tournament_sizes=t_sizes_final,\n",
    "#                             elitism_strategy=scen_config['elitism_strategy'],\n",
    "#                             selector_strategy=scen_config['selector'],\n",
    "#                             survival_strategy=scen_config['survival'],\n",
    "#                             n_elites=scen_config['n_elites'],\n",
    "#                             # use winner params found\n",
    "#                             pop_size=best_params['pop_size'],\n",
    "#                             n_iter=best_params['n_iter'],\n",
    "#                             p_xo=best_params['p_xo'],\n",
    "#                             prob_const=best_params['prob_const'],\n",
    "#                             max_depth=best_params['max_depth'],\n",
    "#                             verbose=0, \n",
    "#                             log_level=5,\n",
    "#                             log_path=log_path,\n",
    "#                             n_jobs=1\n",
    "#                         )\n",
    "#                         print(f\"        > Log saved at: {log_path}\")\n",
    "                    \n",
    "#                     except Exception as e:\n",
    "#                         print(f\"      [Outer Error] {e}\")\n",
    "#                         import traceback\n",
    "#                         traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import scipy.stats as sp\n",
    "import scikit_posthocs as sp_post\n",
    "\n",
    "# configuration\n",
    "LOG_DIR = './log_mo'       # where logs are stored\n",
    "OUTPUT_DIR = './plots_mo'  #where I'll save the plots\n",
    "\n",
    "COLORS = {\n",
    "    'Train_RMSE': '#1f77b4',\n",
    "    'Test_RMSE': '#ff7f0e',\n",
    "    'Std_Dev': '#9467bd',\n",
    "    'Size': '#2ca02c',\n",
    "    'Features': '#d62728',\n",
    "    'NAO': '#17becf',\n",
    "    'NAOC': '#e377c2'\n",
    "}\n",
    "\n",
    "OBJ_MAP = {\n",
    "    0: \"RMSE\",\n",
    "    1: \"Size\",\n",
    "    2: \"Features\",\n",
    "    3: \"NAO\",\n",
    "    4: \"NAOC\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms a column with \"5.1|10.2\" strings into a DataFrame of floats\n",
    "def parse_fitness_column(series, log_file_name=\"Unknown\", allow_na=False):\n",
    "    # deal with NaN values first\n",
    "    if series.isna().any():\n",
    "        if allow_na:\n",
    "            # if it is allowed, return None\n",
    "            return None\n",
    "        else:\n",
    "            # if it is mandatory (like RMSE), NaN is a critical error\n",
    "            raise ValueError(\n",
    "                f\"[CRITICAL ERROR] Found NaN in mandatory column in file '{log_file_name}'. \"\n",
    "                \"This means the Elite was not evaluated correctly.\"\n",
    "            )\n",
    "\n",
    "    # Convert to string for uniform processing\n",
    "    series_str = series.astype(str)\n",
    "\n",
    "    # Handle the literal string \"N/A\" (in case pandas did not convert it to NaN)\n",
    "    if (series_str == \"N/A\").any():\n",
    "        if allow_na:\n",
    "            return None\n",
    "        else:\n",
    "            error_idx = series_str[series_str == \"N/A\"].index[0]\n",
    "            raise ValueError(\n",
    "                f\"[CRITICAL ERROR] Found 'N/A' string in file '{log_file_name}' at line {error_idx}.\"\n",
    "            )\n",
    "\n",
    "    # try to split and convert to float\n",
    "    try:\n",
    "        return series_str.str.split('|', expand=True).astype(float)\n",
    "    except ValueError as e:\n",
    "        raise ValueError(f\"[CRITICAL] Could not convert fitness to numbers in {log_file_name}. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data from all logs\n",
    "def load_data_for_analysis(dataset_name, objectives_key, view_mode='main'):\n",
    "    target_path = os.path.join(LOG_DIR, dataset_name)\n",
    "    \n",
    "    if not os.path.exists(target_path):\n",
    "        print(f\"[ERROR] Folder not found: {target_path}\")\n",
    "        return None, None\n",
    "\n",
    "    # Find all scenarios\n",
    "    scenarios = [d for d in os.listdir(target_path) if os.path.isdir(os.path.join(target_path, d))]\n",
    "    \n",
    "    history_data = {} \n",
    "    final_rows = []   \n",
    "\n",
    "    print(f\"Logs from {dataset_name} ({objectives_key}) - View: {view_mode.upper()}\")\n",
    "\n",
    "    for scen in scenarios:\n",
    "        pattern = os.path.join(target_path, scen, objectives_key, \"fold_*\", \"execution_log.csv\")\n",
    "        files = glob.glob(pattern)\n",
    "        \n",
    "        if not files: continue\n",
    "        \n",
    "        history_data[scen] = []\n",
    "        \n",
    "        for f in files:\n",
    "            try:\n",
    "                df = pd.read_csv(f)\n",
    "                \n",
    "                # --- INDEX MAPPING (Based on MOGP Logger) ---                \n",
    "                # 0: Algo, 1: UUID, 2: Dataset, 3: Seed\n",
    "                # 4: Gen, 5: Main Train Fit, 6: Time, 7: Main Nodes\n",
    "                # 8: Main Test Fit, 9: RMSE Elite Test Fit, 10: RMSE Elite Nodes\n",
    "                # 11: Std Dev, 12: Ideal Point\n",
    "\n",
    "                idx_offset = 4 \n",
    "                \n",
    "                gen_idx = 0 + idx_offset  # 4\n",
    "                std_idx = 7 + idx_offset  # 11\n",
    "                ideal_idx = 8 + idx_offset # 12               \n",
    "\n",
    "                if view_mode == 'rmse':\n",
    "                    # RMSE View: Main Train and RMSE Elite Test\n",
    "                    train_idx = 1 + idx_offset \n",
    "                    test_idx = 5 + idx_offset\n",
    "                    size_idx = 6 + idx_offset\n",
    "                else:\n",
    "                    # Main View: Main Train + Main Test\n",
    "                    train_idx = 1 + idx_offset\n",
    "                    test_idx = 4 + idx_offset\n",
    "                    size_idx = 3 + idx_offset\n",
    "                \n",
    "                # Parse Fitness Columns    \n",
    "                train_fits = parse_fitness_column(df.iloc[:, train_idx], f, allow_na=False)\n",
    "                test_fits = parse_fitness_column(df.iloc[:, test_idx], f, allow_na=False)\n",
    "                \n",
    "                # Ideal Point (allow_na=True because it is N/A for most scenarios)\n",
    "                ideal_fits = parse_fitness_column(df.iloc[:, ideal_idx], f, allow_na=True)\n",
    "\n",
    "                # Clean DataFrame\n",
    "                clean_df = pd.DataFrame({\n",
    "                    'Generation': df.iloc[:, gen_idx],\n",
    "                    'Std_RMSE': df.iloc[:, std_idx]\n",
    "                })\n",
    "                \n",
    "                # Add Train Metrics\n",
    "                if train_fits is not None:\n",
    "                    for c in train_fits.columns:\n",
    "                        clean_df[f'Train_{OBJ_MAP.get(c, c)}'] = train_fits[c]\n",
    "                \n",
    "                # Add Test Metrics\n",
    "                if test_fits is not None:\n",
    "                    for c in test_fits.columns:\n",
    "                        clean_df[f'Test_{OBJ_MAP.get(c, c)}'] = test_fits[c]\n",
    "                \n",
    "                clean_df['Test_Size'] = df.iloc[:, size_idx]\n",
    "                \n",
    "                # Add Ideal Point if available\n",
    "                if ideal_fits is not None:\n",
    "                    clean_df['Ideal_RMSE'] = ideal_fits[0]\n",
    "\n",
    "                history_data[scen].append(clean_df)\n",
    "                \n",
    "                # Data for Boxplots (Last Generation)\n",
    "                last_row = clean_df.iloc[-1]\n",
    "                try: fold_num = int(f.split(os.sep)[-2].split('_')[-1])\n",
    "                except: fold_num = 0\n",
    "\n",
    "                row_dict = {'Dataset': dataset_name, 'Scenario': scen, 'Fold': fold_num}\n",
    "                for col in clean_df.columns:\n",
    "                    if col.startswith('Train_') or col.startswith('Test_'):\n",
    "                        row_dict[col] = last_row[col]\n",
    "                final_rows.append(row_dict)\n",
    "            except Exception as e:\n",
    "                print(f\"  [WARN] Could not process file {f}: {e}\")\n",
    "\n",
    "    final_df = pd.DataFrame(final_rows)\n",
    "    return history_data, final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convergence(dataset_name, objectives_key, history_data, view_mode):\n",
    "    save_dir = os.path.join(OUTPUT_DIR, dataset_name, objectives_key)\n",
    "    if not os.path.exists(save_dir): os.makedirs(save_dir)\n",
    "\n",
    "    for scen, folds_list in history_data.items():\n",
    "        if not folds_list: continue\n",
    "\n",
    "        sample_df = folds_list[0]\n",
    "        metrics = [c.replace(\"Test_\", \"\") for c in sample_df.columns if c.startswith(\"Test_\")]\n",
    "        min_len = min([len(df) for df in folds_list])\n",
    "        generations = np.arange(min_len)\n",
    "\n",
    "        fig = make_subplots(\n",
    "            rows=1, cols=3,\n",
    "            subplot_titles=(\"Performance (RMSE)\", \"Structural Complexity\", \"Pop RMSE Std Dev\"),\n",
    "            horizontal_spacing=0.08\n",
    "        )\n",
    "\n",
    "        def add_trace(col, name, color, idx, show_leg=True):\n",
    "            if col not in sample_df.columns: return\n",
    "\n",
    "            data = np.array([df[col].values[:min_len] for df in folds_list])\n",
    "            median = np.median(data, axis=0)\n",
    "            q1 = np.percentile(data, 15, axis=0)\n",
    "            q3 = np.percentile(data, 85, axis=0)\n",
    "\n",
    "            # IQR shading\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=np.concatenate([generations, generations[::-1]]),\n",
    "                y=np.concatenate([q3, q1[::-1]]),\n",
    "                fill='toself', fillcolor=color, opacity=0.15,\n",
    "                line=dict(width=0), showlegend=False), row=1, col=idx)\n",
    "\n",
    "            # Median Line\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=generations, y=median, mode='lines', name=name,\n",
    "                line=dict(color=color)), row=1, col=idx)\n",
    "\n",
    "        # Plot 1: Performance\n",
    "        add_trace('Train_RMSE', 'Train RMSE', COLORS['Train_RMSE'], 1)\n",
    "        add_trace('Test_RMSE', 'Test RMSE', COLORS['Test_RMSE'], 1)\n",
    "        \n",
    "        # Add Ideal Point line if it exists\n",
    "        if 'Ideal_RMSE' in sample_df.columns:\n",
    "             add_trace('Ideal_RMSE', 'Ideal Point', 'black', 1, show_leg=True)\n",
    "\n",
    "        # Plot 2: Complexity\n",
    "        add_trace('Test_Size', 'Size', COLORS['Size'], 2)\n",
    "        for metric in [m for m in metrics if m != \"RMSE\" and m != \"Size\"]:\n",
    "            color = COLORS.get(metric, '#333333')\n",
    "            add_trace(f'Test_{metric}', metric, color, 2)\n",
    "\n",
    "        # Plot 3: Std Dev\n",
    "        add_trace('Std_RMSE', 'RMSE Std Dev', COLORS['Std_Dev'], 3, show_leg=False)\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=f\"Convergence Analysis: {scen} ({dataset_name} | {objectives_key} | {view_mode.upper()})\",\n",
    "            height=500, width=1400, template=\"plotly_white\",\n",
    "            legend=dict(orientation=\"h\", y=-0.15, x=0.5, xanchor=\"center\")\n",
    "        )\n",
    "        \n",
    "        file_path = os.path.join(save_dir, f\"convergence_{scen}_VIEW_{view_mode.upper()}.png\")\n",
    "        fig.write_image(file_path)\n",
    "        print(f\"  -> Saved convergence: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxplots(dataset_name, objectives_key, final_df, view_mode):\n",
    "    save_dir = os.path.join(OUTPUT_DIR, dataset_name, objectives_key)\n",
    "    \n",
    "    if final_df.empty: return\n",
    "\n",
    "    df_melt = final_df.melt(\n",
    "        id_vars=['Scenario'], \n",
    "        value_vars=['Train_RMSE', 'Test_RMSE'],\n",
    "        var_name='Type', value_name='RMSE'\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    sns.boxplot(\n",
    "        data=df_melt, x='Scenario', y='RMSE', hue='Type',\n",
    "        palette=[COLORS['Train_RMSE'], COLORS['Test_RMSE']],\n",
    "        width=0.6, linewidth=1.2, showfliers=False \n",
    "    )\n",
    "    sns.stripplot(\n",
    "        data=df_melt, x='Scenario', y='RMSE', hue='Type',\n",
    "        dodge=True, color='black', alpha=0.3, size=3, legend=False\n",
    "    )\n",
    "\n",
    "    plt.title(f'Final RMSE Distribution: {dataset_name} ({view_mode.upper()})', fontsize=14)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    filename = f\"boxplot_rmse_VIEW_{view_mode.upper()}.png\"\n",
    "    plt.savefig(os.path.join(save_dir, filename), dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"  -> Saved RMSE boxplot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_stats(dataset_name, objectives_key, final_df, view_mode, save_dir):\n",
    "    if final_df.empty: return\n",
    "    \n",
    "    try:\n",
    "        pivot_df = final_df.pivot(index='Fold', columns='Scenario', values='Test_RMSE')\n",
    "        \n",
    "        output_txt = [f\"=== Stats: {dataset_name} ({objectives_key}) ===\\n\"]\n",
    "        output_txt.append(pivot_df.describe().to_string() + \"\\n\\n\")\n",
    "        \n",
    "        stat, p = sp.friedmanchisquare(*[pivot_df[col] for col in pivot_df.columns])\n",
    "        output_txt.append(f\"Friedman Test:\\nStatistic: {stat:.4f}, p-value: {p:.6f}\\n\")\n",
    "        \n",
    "        if p < 0.05:\n",
    "            output_txt.append(\"\\nSignificant differences found. Nemenyi Post-Hoc:\\n\")\n",
    "            nemenyi = sp_post.posthoc_nemenyi_friedman(pivot_df)\n",
    "            output_txt.append(nemenyi.to_string())\n",
    "        \n",
    "        filename = f\"stats_VIEW_{view_mode.upper()}.txt\"\n",
    "        with open(os.path.join(save_dir, filename), \"w\") as f:\n",
    "            f.write(\"\".join(output_txt))\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Stats Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import KFold, ParameterGrid\n",
    "from slim_gsgp.main_mo_gp import mo_gp\n",
    "# from slim_gsgp.datasets.data_loader import (\n",
    "#      load_efficiency_cooling, load_ld50\n",
    "# )\n",
    "# DATASETS = {\n",
    "#     #'Cooling': load_efficiency_cooling,\n",
    "#     'Toxicity': load_ld50,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Toxicity [2_Objs] ---\n",
      "Logs from Toxicity (2_Objs) - View: MAIN\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2_Objs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3_Objs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5_Objs\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m---> 20\u001b[0m         \u001b[43mgenerate_all_plots\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[33], line 4\u001b[0m, in \u001b[0;36mgenerate_all_plots\u001b[1;34m(dataset_name, objectives_key, view_mode)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_all_plots\u001b[39m(dataset_name, objectives_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2_Objs\u001b[39m\u001b[38;5;124m'\u001b[39m, view_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Processing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobjectives_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     history_data, final_df \u001b[38;5;241m=\u001b[39m \u001b[43mload_data_for_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjectives_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mview_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m final_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m final_df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo data found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[28], line 27\u001b[0m, in \u001b[0;36mload_data_for_analysis\u001b[1;34m(dataset_name, objectives_key, view_mode)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 27\u001b[0m         df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;66;03m# --- INDEX MAPPING (Based on MOGP Logger) ---                \u001b[39;00m\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;66;03m# 0: Algo, 1: UUID, 2: Dataset, 3: Seed\u001b[39;00m\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;66;03m# 4: Gen, 5: Main Train Fit, 6: Time, 7: Main Nodes\u001b[39;00m\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;66;03m# 8: Main Test Fit, 9: RMSE Elite Test Fit, 10: RMSE Elite Nodes\u001b[39;00m\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;66;03m# 11: Std Dev, 12: Ideal Point\u001b[39;00m\n\u001b[0;32m     35\u001b[0m         idx_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m \n",
      "File \u001b[1;32mc:\\Users\\filip\\anaconda3\\envs\\TM\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\filip\\anaconda3\\envs\\TM\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\filip\\anaconda3\\envs\\TM\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\filip\\anaconda3\\envs\\TM\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\filip\\anaconda3\\envs\\TM\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "File \u001b[1;32mc:\\Users\\filip\\anaconda3\\envs\\TM\\lib\\codecs.py:309\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.__init__\u001b[1;34m(self, errors)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mBufferedIncrementalDecoder\u001b[39;00m(IncrementalDecoder):\n\u001b[0;32m    304\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03m    This subclass of IncrementalDecoder can be used as the baseclass for an\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03m    incremental decoder if the decoder must be able to handle incomplete\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;124;03m    byte sequences.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 309\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    310\u001b[0m         IncrementalDecoder\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, errors)\n\u001b[0;32m    311\u001b[0m         \u001b[38;5;66;03m# undecoded input that is kept between calls to decode()\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def generate_all_plots(dataset_name, objectives_key='2_Objs', view_mode='main'):\n",
    "    print(f\"\\n--- Processing {dataset_name} [{objectives_key}] ---\")\n",
    "    \n",
    "    history_data, final_df = load_data_for_analysis(dataset_name, objectives_key, view_mode)\n",
    "    \n",
    "    if final_df is None or final_df.empty:\n",
    "        print(\"No data found.\")\n",
    "        return\n",
    "\n",
    "    plot_convergence(dataset_name, objectives_key, history_data, view_mode)\n",
    "    plot_boxplots(dataset_name, objectives_key, final_df, view_mode)\n",
    "    \n",
    "    save_dir = os.path.join(OUTPUT_DIR, dataset_name, objectives_key)\n",
    "    run_stats(dataset_name, objectives_key, final_df, view_mode, save_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for ds in DATASETS: # DATASETS dict defined in run_experiments, or hardcode list here\n",
    "        for obj in ['2_Objs', '3_Objs', '5_Objs']:\n",
    "            for mode in ['main', 'rmse']:\n",
    "                generate_all_plots(ds, obj, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATASETS A USAR, SENDO QUE POR AGORA QUEREMOS S UM GRFICO PARA CADA #APAGAR\n",
    "\"\"\"\n",
    "load_resid_build_sale_price(X_y=True):\n",
    "    Loads and returns the RESIDNAME data set (regression). Taken from https://archive.ics.uci.edu/dataset/437/residential+building+data+set\n",
    "\n",
    "load_istanbul(X_y=True):\n",
    "    Loads and returns the Istanbul data set (regression). Taken from https://docs.1010data.com/MachineLearningExamples/IstanbulDataSet.html.\n",
    "\n",
    "load_airfoil(X_y=True):\n",
    "    - Number of data instances: 1503;\n",
    "    - Number of input features: 5;\n",
    "    - Target's range: [103.38-140.987].\n",
    "\n",
    "load_bike_sharing(X_y=True):\n",
    "    - Number of data instances: 17389;\n",
    "    - Number of input features: 13;\n",
    "    - Target's range: [22, 8714].                            x\n",
    "\n",
    "load_boston(X_y=True):\n",
    "    - Number of data instances: 506;\n",
    "    - Number of input features: 13;\n",
    "    - Target's range: [5, 50].\n",
    "\n",
    " load_concrete_slump\n",
    "    - Number of data instances: 103;\n",
    "    - Number of input features: 7;\n",
    "    - Target's range: [0, 29].\n",
    "\n",
    "load_concrete_strength(X_y=True):\n",
    "    - Number of data instances: 1005;\n",
    "    - Number of input features: 8;\n",
    "    - Target's range: [2.331807832, 82.5992248].\n",
    "\n",
    "load_diabetes(X_y=True):\n",
    "    - Number of data instances: 442;\n",
    "    - Number of input features: 10;\n",
    "    - Target's range: [25, 346].\n",
    "\n",
    "load_efficiency_heating(X_y=True):\n",
    "    - Number of data instances: 768;\n",
    "    - Number of input features: 8;\n",
    "    - Target's range: [6.01, 43.1].\n",
    "\n",
    "load_efficiency_cooling(X_y=True):<-------------- usar este\n",
    "    - Number of data instances: 768;\n",
    "    - Number of input features: 8;\n",
    "    - Target's range: [10.9, 48.03].\n",
    "\n",
    "load_forest_fires(X_y=True):\n",
    "    - Number of data instances: 513;\n",
    "    - Number of input features: 43;\n",
    "    - Target's range: [0.0, 6.995619625423205].\n",
    "\n",
    "load_parkinson_updrs(X_y=True):\n",
    "    - Number of data instances: 5875;                           x\n",
    "    - Number of input features: 19;\n",
    "    - Target's range: [7.0, 54.992].\n",
    "\n",
    "load_ld50(X_y=True):  <-----------------usar este\n",
    "    - Number of data instances: 234;\n",
    "    - Number of input features: 626;\n",
    "    - Target's range: [0.25, 8900.0].\n",
    "\n",
    "load_ppb(X_y=True):                              --------------usar este\n",
    "    - Number of data instances: 131;\n",
    "    - Number of input features: 626;\n",
    "    - Target's range: [0.5, 100.0]\n",
    "\n",
    "load_bioav(X_y=True):                             --------------usar este\n",
    "    - Number of data instances: 358;\n",
    "    - Number of input features: 241;\n",
    "    - Target's range: [0.4, 100.0].\n",
    "\"\"\"\n",
    "#APAGAR: hyperparameters\n",
    "\"\"\"\n",
    "(function) def mo_gp(\n",
    "    X_train: Tensor,\n",
    "    y_train: Tensor,\n",
    "    X_test: Tensor = None,\n",
    "    y_test: Tensor = None,\n",
    "    dataset_name: str = None,\n",
    "    pop_size: int = gp_parameters[\"pop_size\"],\n",
    "    selector_strategy: str = \"nested_tournament\",\n",
    "    survival_strategy: str = \"nsga2\",\n",
    "    offspring_size: int | None = None,\n",
    "    n_iter: int = gp_solve_parameters[\"n_iter\"],\n",
    "    p_xo: float = gp_parameters['p_xo'],\n",
    "    n_elites: int = gp_solve_parameters[\"n_elites\"],\n",
    "    max_depth: int | None = gp_solve_parameters[\"max_depth\"],\n",
    "    init_depth: int = gp_pi_init[\"init_depth\"],\n",
    "    log_path: str = None,\n",
    "    seed: int = gp_parameters[\"seed\"],\n",
    "    log_level: int = gp_solve_parameters[\"log\"],\n",
    "    verbose: int = gp_solve_parameters[\"verbose\"],\n",
    "    fitness_functions: list = mo_parameters[\"mo_fitness_functions\"],\n",
    "    minimization_flags: list = mo_parameters[\"mo_minimization_flags\"],\n",
    "    tournament_sizes: list = mo_parameters[\"mo_tournament_sizes\"],\n",
    "    ideal_candidate_values: list | None = None,\n",
    "    initializer: str = gp_parameters[\"initializer\"],\n",
    "    n_jobs: int = gp_solve_parameters[\"n_jobs\"],\n",
    "    prob_const: float = gp_pi_init[\"p_c\"],\n",
    "    tree_functions: list = list(FUNCTIONS.keys()),\n",
    "    tree_constants: list = [float(key.replace(\"constant_\", \"\").replace(\"_\", \"-\")) for key in CONSTANTS],\n",
    "    test_elite: bool = gp_solve_parameters[\"test_elite\"]\n",
    ") -> Any\n",
    "Main function to execute the Multi-Objective Genetic Programming (MOGP) algorithm on specified datasets\n",
    "\n",
    "Parameters\n",
    "X_train: : torch.Tensor\n",
    "Training input data.\n",
    "\n",
    "y_train: : torch.Tensor\n",
    "Training output data.\n",
    "\n",
    "X_test: : torch.Tensor , optional\n",
    "Testing input data.\n",
    "\n",
    "y_test: : torch.Tensor , optional\n",
    "Testing output data.\n",
    "\n",
    "dataset_name : str, optional\n",
    "Dataset name, for logging purposes\n",
    "\n",
    "pop_size : int, optional\n",
    "The population size for the genetic programming algorithm (default is 100).\n",
    "\n",
    "selector_strategy : str, optional\n",
    "The selection strategy for parent selection. Options are \"nested_tournament\" or \"nsga2\" (default is \"nested_tournament\").\n",
    "\n",
    "survival_strategy : str, optional\n",
    "The survival selection strategy. Options are \"nsga2\" or \"generational\" (default is \"nsga2\").\n",
    "\n",
    "offspring_size : int, optional\n",
    "The size of the offspring population to be generated in each generation. If None, it defaults to pop_size.\n",
    "\n",
    "n_iter : int, optional\n",
    "The number of iterations for the genetic programming algorithm (default is 100).\n",
    "\n",
    "p_xo : float, optional\n",
    "The probability of crossover in the genetic programming algorithm. Must be a number between 0 and 1 (default is 0.8).\n",
    "\n",
    "n_elites : int, optional\n",
    "The number of elites.\n",
    "\n",
    "max_depth : int, optional\n",
    "The maximum depth for the GP trees.\n",
    "\n",
    "init_depth : int, optional\n",
    "The depth value for the initial GP trees population.\n",
    "\n",
    "log_path : str, optional\n",
    "The path where is created the log directory where results are saved. Defaults to os.path.join(os.getcwd(), \"log\", \"mo_gp.csv\")\n",
    "\n",
    "seed : int, optional\n",
    "Seed for the randomness\n",
    "\n",
    "log_level : int, optional\n",
    "Level of detail to utilize in logging.\n",
    "\n",
    "verbose : int, optional\n",
    "Level of detail to include in console output.\n",
    "\n",
    "fitness_functions : list, optional\n",
    "A list of fitness function names, one for each objective. (Default is from mo_parameters)\n",
    "\n",
    "minimization_flags : list, optional\n",
    "A list of booleans indicating if each corresponding objective is for minimization (True) or maximization (False). (Default is from mo_parameters)\n",
    "\n",
    "tournament_sizes : list, optional\n",
    "A list of integers defining the tournament size for each objective during Nested Tournament Selection. (Default is from mo_parameters)\n",
    "\n",
    "ideal_candidate_values : list, optional\n",
    "A list of ideal candidate values for each objective to guide elite selection. If None, defaults uses first-objective logic.\n",
    "\n",
    "initializer : str, optional\n",
    "The strategy for initializing the population (e.g., \"grow\", \"full\", \"rhh\").\n",
    "\n",
    "n_jobs : int, optional\n",
    "Number of parallel jobs to run (default is 1).\n",
    "\n",
    "prob_const : float, optional\n",
    "The probability of a constant being chosen rather than a terminal in trees creation (default: 0.2).\n",
    "\n",
    "tree_functions : list, optional\n",
    "List of allowed functions that can appear in the trees. Check documentation for the available functions.\n",
    "\n",
    "tree_constants : list, optional\n",
    "List of constants allowed to appear in the trees.\n",
    "\n",
    "test_elite : bool, optional\n",
    "Whether to test the elite individual on the test set after each generation.\n",
    "\n",
    "Returns\n",
    "MultiObjectiveTree\n",
    "Returns the best individual according to the tracking strategy at the last generation.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Tornament Size could be more \"flexible\"\n",
    "For instance, 5% of pop_size (but has to be >1)\n",
    "    t_size_val = int(params['pop_size'] * 0.05) \n",
    "    t_sizes = [max(2, t_size_val)] * n_objs\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
